{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import statistics\n",
    "import torch.distributions as tdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_Net, self).__init__()\n",
    "        self.l1 = nn.Linear(24,64)\n",
    "        self.l2 = nn.Linear(64,64)\n",
    "        self.l3 = nn.Linear(64,2)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.l1(state))\n",
    "        state = F.relu(self.l2(state))\n",
    "        state = F.tanh(self.l3(state))\n",
    "        return state\n",
    "\n",
    "    \n",
    "policy=Policy_Net()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.0003)\n",
    "\n",
    "policy_1=Policy_Net()\n",
    "optimizer_1 = optim.Adam(policy_1.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic_Net, self).__init__()\n",
    "        self.l1 = nn.Linear(48,64)\n",
    "        self.l2 = nn.Linear(64,64)\n",
    "        self.l3 = nn.Linear(64,2)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.l1(state))\n",
    "        state = F.relu(self.l2(state))\n",
    "        state = F.tanh(self.l3(state))\n",
    "        return state\n",
    "    \n",
    "\n",
    "value_function=Critic_Net()\n",
    "optimizer_critic = optim.Adam(value_function.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantage(rewards, critic, states, l = .96):\n",
    "  value_est = critic(states)\n",
    "  value_est_0 = value_est[:-1]\n",
    "  value_est_1 = value_est[1:]\n",
    "  rewards = torch.tensor(rewards)\n",
    "  advantage = rewards[:-1] + discount_rate * value_est_1 - value_est_0\n",
    "  advantage = torch.cat((advantage,(rewards[-1]).view(1,2)))\n",
    "  return advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_loss(rewards, critic, states):\n",
    "  value_est = critic(states)\n",
    "  rewards = torch.tensor(rewards)\n",
    "  return F.l1_loss(rewards,value_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount, epsilon, var, actor):\n",
    "\n",
    "    rewards = rewards[:,actor].view(-1,1)\n",
    "    # convert states to policy (or probability)\n",
    "    new_total_probs = policy(states)\n",
    "    dist = tdist.multivariate_normal.MultivariateNormal(new_total_probs, torch.eye(2) * var)\n",
    "    new_probs = dist.log_prob(actions)\n",
    "\n",
    "    ratio = torch.exp(new_probs-old_probs)\n",
    "    \n",
    "    clipped_loss = torch.where(ratio > torch.tensor(1+epsilon), torch.tensor(1+epsilon), ratio)\n",
    "    clipped_loss = torch.where(clipped_loss < torch.tensor(1-epsilon),  torch.tensor(1-epsilon), clipped_loss)\n",
    "    clipped_loss = torch.min(ratio * rewards, clipped_loss * rewards)\n",
    "    #return torch.mean(torch.clamp(new_probs/old_probs * rewards, 1-epsilon * rewards, 1+epsilon * rewards))\n",
    "\n",
    "    \n",
    "    return torch.mean(clipped_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env, policy, policy_1, Max_Time, var, training_mode = True):\n",
    "  env_info = env.reset(train_mode=training_mode)[brain_name]\n",
    "  s0 = prepro(env_info.vector_observations)\n",
    "  states = list()\n",
    "  actions = list()\n",
    "  rewards = list()\n",
    "  old_probs = list()\n",
    "\n",
    "  states_1 = list()\n",
    "  actions_1 = list()\n",
    "  rewards_1 = list()\n",
    "  old_probs_1 = list()\n",
    "    \n",
    "  \n",
    "  for step in range(Max_Time):\n",
    "    if 1 == 0:\n",
    "      action = 0\n",
    "    else:\n",
    "      #Gets Action\n",
    "      probs = policy((s0[0]-states_mean)/states_std)\n",
    "      probs = tdist.multivariate_normal.MultivariateNormal(probs, torch.eye(2) * var)\n",
    "    \n",
    "                                                           \n",
    "      probs_1 = policy_1((s0[1]-states_mean)/states_std)\n",
    "      probs_1 = tdist.multivariate_normal.MultivariateNormal(probs_1, torch.eye(2) * var)\n",
    "    \n",
    "      action = probs.sample()\n",
    "      action_1 = probs_1.sample()\n",
    "                                                           \n",
    "      probs = probs.log_prob(action)\n",
    "      probs_1 = probs_1.log_prob(action_1)\n",
    "                                                           \n",
    "      #Does Action\n",
    "    env_info = env.step(torch.stack((torch.clamp(action,-1,1),torch.clamp(action_1,-1,1)),0).detach().numpy())[brain_name]           # send all actions to tne environment\n",
    "    state = env_info.vector_observations         # get next state (for each agent)\n",
    "    reward = env_info.rewards                         # get reward (for each agent)\n",
    "    done = env_info.local_done       \n",
    "    #Adds to experience\n",
    "    state = prepro(state)\n",
    "    if step > -1:\n",
    "      states.append(s0[0])\n",
    "      actions.append(action)\n",
    "      rewards.append(reward)\n",
    "      old_probs.append(probs)\n",
    "                                                           \n",
    "      states_1.append(s0[1])\n",
    "      actions_1.append(action_1)\n",
    "      rewards_1.append(reward)\n",
    "      old_probs_1.append(probs_1)   \n",
    "    s0 = state\n",
    "    if np.any(done):\n",
    "      break\n",
    "  scores = deepcopy(rewards)\n",
    "  discount_scores = torch.tensor(deepcopy(rewards))\n",
    "  #rewards = torch.tensor(rewards)\n",
    "  rewards = get_advantage(rewards, value_function,  torch.cat((torch.stack(states),torch.stack(states_1)),1))\n",
    "  for i in reversed(range(len(rewards)-1)):\n",
    "    rewards[i] += discount_rate *0.95* rewards[i + 1]\n",
    "    discount_scores[i] += discount_rate  * discount_scores[i+1]\n",
    "    #if len(advantage_rewards[-1][i:]) > horizon_len:\n",
    "    #  advantage_rewards[-1][i] += -1 *  advantage_rewards[-1][i + horizon_len - 1] * discount**horizon_len \n",
    "  #advantage_rewards[-1] = advantage_rewards[-1][:-horizon_len]   \n",
    "  rewards = deepcopy(discount_scores)\n",
    "  return old_probs, old_probs_1, states, states_1, actions, actions_1, rewards, scores, discount_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmuralikrishnan\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 game average: 0.016000\n",
      "0.01600000023841858\n",
      "50 game average: 0.031400\n",
      "0.023700000364333392\n",
      "50 game average: 0.015200\n",
      "0.020866666994988917\n",
      "50 game average: 0.007800\n",
      "0.017600000277161597\n",
      "50 game average: 0.007800\n",
      "0.015640000246465207\n",
      "50 game average: 0.029000\n",
      "0.017866666950285433\n",
      "50 game average: 0.013800\n",
      "0.01728571455925703\n",
      "50 game average: 0.015800\n",
      "0.017100000269711018\n",
      "50 game average: 0.023400\n",
      "0.017800000280969674\n",
      "50 game average: 0.029600\n",
      "0.01898000030219555\n",
      "50 game average: 0.011600\n",
      "0.018309091200882737\n",
      "50 game average: 0.025600\n",
      "0.018916666967173416\n",
      "50 game average: 0.019200\n",
      "0.01893846184015274\n",
      "50 game average: 0.017400\n",
      "0.018828571728829825\n",
      "50 game average: 0.023600\n",
      "0.019146666971345743\n",
      "50 game average: 0.011600\n",
      "0.0186750002973713\n",
      "50 game average: 0.017400\n",
      "0.018600000296445453\n",
      "50 game average: 0.014000\n",
      "0.018344444736010497\n",
      "50 game average: 0.035600\n",
      "0.019252631883872183\n",
      "50 game average: 0.017400\n",
      "0.01916000030376017\n",
      "50 game average: 0.025600\n",
      "0.01946666697483687\n",
      "50 game average: 0.021400\n",
      "0.019554545764218677\n",
      "50 game average: 0.015800\n",
      "0.019391304654595643\n",
      "50 game average: 0.015800\n",
      "0.01924166697077453\n",
      "50 game average: 0.025400\n",
      "0.0194880003079772\n",
      "50 game average: 0.021600\n",
      "0.01956923107831524\n",
      "50 game average: 0.017600\n",
      "0.019496296604198438\n",
      "50 game average: 0.013800\n",
      "0.019292857447373017\n",
      "50 game average: 0.005400\n",
      "0.01881379340100905\n",
      "50 game average: 0.027000\n",
      "0.019086666968961555\n",
      "50 game average: 0.023400\n",
      "0.01922580675612534\n",
      "50 game average: 0.021400\n",
      "0.01929375030566007\n",
      "50 game average: 0.023400\n",
      "0.019418182125823063\n",
      "50 game average: 0.025400\n",
      "0.019594117957441246\n",
      "50 game average: 0.017600\n",
      "0.019537143166576114\n",
      "50 game average: 0.019400\n",
      "0.019533333642822174\n",
      "50 game average: 0.019200\n",
      "0.019524324633986564\n",
      "50 game average: 0.013800\n",
      "0.01937368451764709\n",
      "50 game average: 0.023600\n",
      "0.01948205159069636\n",
      "50 game average: 0.016000\n",
      "0.019395000306889416\n",
      "50 game average: 0.023400\n",
      "0.019492683235283303\n",
      "50 game average: 0.017200\n",
      "0.01943809554601709\n",
      "50 game average: 0.020000\n",
      "0.019451163098389325\n",
      "50 game average: 0.021600\n",
      "0.019500000308352437\n",
      "50 game average: 0.023600\n",
      "0.01959111142075724\n",
      "50 game average: 0.027400\n",
      "0.019760869878303746\n",
      "50 game average: 0.035600\n",
      "0.020097872658454356\n",
      "50 game average: 0.021200\n",
      "0.020120833651938785\n",
      "50 game average: 0.037400\n",
      "0.0204734697116881\n",
      "50 game average: 0.021600\n",
      "0.02049600032418966\n",
      "50 game average: 0.023400\n",
      "0.020552941501578863\n",
      "50 game average: 0.017200\n",
      "0.0204884618628197\n",
      "50 game average: 0.023800\n",
      "0.020550943721296653\n",
      "50 game average: 0.019400\n",
      "0.020529629954447348\n",
      "50 game average: 0.043200\n",
      "0.02094181851297617\n",
      "50 game average: 0.025800\n",
      "0.021028571761479334\n",
      "50 game average: 0.028000\n",
      "0.02115087752736974\n",
      "50 game average: 0.019800\n",
      "0.021127586540733947\n",
      "50 game average: 0.025400\n",
      "0.021200000334973053\n",
      "50 game average: 0.023800\n",
      "0.021243333668758472\n",
      "50 game average: 0.019800\n",
      "0.02121967246603282\n",
      "50 game average: 0.019800\n",
      "0.021196774527911216\n",
      "50 game average: 0.017800\n",
      "0.021142857476241058\n",
      "50 game average: 0.017200\n",
      "0.021081250333227216\n",
      "50 game average: 0.023800\n",
      "0.02112307725674831\n",
      "50 game average: 0.021400\n",
      "0.021127273061058736\n",
      "50 game average: 0.027600\n",
      "0.02122388093217985\n",
      "50 game average: 0.027400\n",
      "0.021314706218921963\n",
      "50 game average: 0.017800\n",
      "0.021263768451585285\n",
      "50 game average: 0.019000\n",
      "0.02123142890685371\n",
      "50 game average: 0.030800\n",
      "0.021366197520893224\n",
      "50 game average: 0.024800\n",
      "0.021413889227745432\n",
      "50 game average: 0.027200\n",
      "0.021493151025106644\n",
      "50 game average: 0.033600\n",
      "0.021656757099302235\n",
      "50 game average: 0.037400\n",
      "0.021866667012373605\n",
      "50 game average: 0.036800\n",
      "0.02206315824369851\n",
      "50 game average: 0.027200\n",
      "0.02212987047995066\n",
      "50 game average: 0.033400\n",
      "0.022274359326618604\n",
      "50 game average: 0.031000\n",
      "0.02238481048117333\n",
      "50 game average: 0.021400\n",
      "0.02237250035442412\n",
      "50 game average: 0.025400\n",
      "0.022409876898207046\n",
      "50 game average: 0.043000\n",
      "0.022660975968692362\n",
      "50 game average: 0.017600\n",
      "0.022600000357951025\n",
      "50 game average: 0.027400\n",
      "0.022657143215959272\n",
      "50 game average: 0.033000\n",
      "0.022778823890230233\n",
      "50 game average: 0.019800\n",
      "0.022744186406651903\n",
      "50 game average: 0.025000\n",
      "0.022770115303239604\n",
      "50 game average: 0.027400\n",
      "0.02282272763423283\n",
      "50 game average: 0.029200\n",
      "0.022894382385139384\n",
      "50 game average: 0.037400\n",
      "0.023055555920634005\n",
      "50 game average: 0.023600\n",
      "0.023061538826633286\n",
      "50 game average: 0.031000\n",
      "0.02314782645350889\n",
      "50 game average: 0.035000\n",
      "0.023275269185823778\n",
      "50 game average: 0.021600\n",
      "0.023257447176791252\n",
      "50 game average: 0.031200\n",
      "0.023341053001190486\n",
      "50 game average: 0.035600\n",
      "0.023468750371442488\n",
      "50 game average: 0.041000\n",
      "0.023649484910378136\n",
      "50 game average: 0.029600\n",
      "0.023710204456761783\n",
      "50 game average: 0.025600\n",
      "0.023729293304636623\n",
      "50 game average: 0.033400\n",
      "0.023826000376790762\n",
      "50 game average: 0.031400\n",
      "0.023900990476924004\n",
      "50 game average: 0.029200\n",
      "0.023952941555237653\n",
      "50 game average: 0.035000\n",
      "0.024060194555272175\n",
      "50 game average: 0.035000\n",
      "0.02416538499761373\n",
      "50 game average: 0.027400\n",
      "0.024196190858880678\n",
      "50 game average: 0.039200\n",
      "0.024337736233928292\n",
      "50 game average: 0.025600\n",
      "0.024349533095259532\n",
      "50 game average: 0.021600\n",
      "0.024324074458606817\n",
      "50 game average: 0.027600\n",
      "0.024354128825281738\n",
      "50 game average: 0.021200\n",
      "0.02432545493035154\n",
      "50 game average: 0.035400\n",
      "0.024425225611608307\n",
      "50 game average: 0.029600\n",
      "0.024471428958433013\n",
      "50 game average: 0.033000\n",
      "0.02454690304312822\n",
      "50 game average: 0.030800\n",
      "0.02460175477523814\n",
      "50 game average: 0.025000\n",
      "0.024605217780755913\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "episode = 10000\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "var = 1.0\n",
    "tmax = 32000\n",
    "SGD_epoch = 10\n",
    "SGD_epoch_2 = 100\n",
    "num_traj = 50\n",
    "batch_size = 32\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    train_var = var\n",
    "    old_probs, old_probs_1, states, states_1, actions, actions_1, rewards, scores, discount_scores = list(),list(),list(), list(), list(), list(), torch.tensor(list()) ,list(), torch.tensor(list())\n",
    "    for traj in range(num_traj):\n",
    "      old_probs_, old_probs_1_, states_, states_1_, actions_, actions_1_, rewards_, scores_, discount_scores_ = collect_trajectories(env, policy, policy_1, tmax, var)\n",
    "      \n",
    "      old_probs = old_probs + old_probs_\n",
    "      states = states + states_       \n",
    "      actions = actions + actions_\n",
    "        \n",
    "      old_probs_1 = old_probs_1 + old_probs_1_\n",
    "      states_1 = states_1 + states_1_\n",
    "      actions_1 = actions_1 + actions_1_\n",
    "\n",
    "      rewards = torch.cat((rewards,rewards_))\n",
    "      discount_scores = torch.cat((discount_scores,discount_scores_))\n",
    "      scores = scores + scores_\n",
    "      mean_rewards.append(np.max(sum(np.array(scores_))))\n",
    "    #old_probs, states, actions, rewards, scores = collect_trajectories(env, policy, tmax)\n",
    "        \n",
    "    #Normalize Rewards\n",
    "    \n",
    "\n",
    "    states_mean = (torch.mean(torch.stack(states),0) + 3 * states_mean)/4\n",
    "    states_std = (torch.std(torch.stack(states),0) + 3 * states_std)/4\n",
    "    #states = (torch.stack(states)-states_mean)/(states_std+0.0000001)\n",
    "    states = (torch.stack(states)-states_mean)/states_std\n",
    "    #states_1_mean = torch.mean(torch.stack(states_1),0)\n",
    "    #states_1_std = torch.std(torch.stack(states_1),0)\n",
    "    #states_1 = (torch.stack(states_1)-states_1_mean)/(states_1_std+0.0000001)\n",
    "    states_1 = (torch.stack(states_1)-states_mean)/states_std\n",
    "    rewards_mean = 0\n",
    "    rewards_std = 1\n",
    "    #rewards_mean = torch.tensor([torch.mean(rewards[:,0]),torch.mean(rewards[:,1])])\n",
    "    #rewards_std = torch.tensor([torch.std(rewards[:,0]),torch.std(rewards[:,1])])\n",
    "    rewards = [(i-rewards_mean)/(rewards_std+0.0001) for i in rewards]\n",
    "\n",
    "    total_rewards = np.sum(scores, axis=0)/num_traj\n",
    "    \n",
    "    actions = torch.stack(actions)\n",
    "    old_probs = torch.tensor(old_probs)\n",
    "    rewards = torch.stack(rewards)\n",
    "    actions_1 = torch.stack(actions_1)\n",
    "    old_probs_1 = torch.tensor(old_probs_1)\n",
    "\n",
    "    \n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        permutation = torch.randperm(len(states))\n",
    "\n",
    "        for i in range(0,len(states), batch_size):\n",
    "\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            old_probs_batch, states_batch,actions_batch,rewards_batch = old_probs[indices], states[indices],actions[indices],rewards[indices]\n",
    "\n",
    "            L = -clipped_surrogate(policy, old_probs_batch, states_batch, actions_batch, rewards_batch, discount_rate, epsilon, var, 0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            L.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            del L\n",
    "        \n",
    "    for _ in range(SGD_epoch):\n",
    "\n",
    "        permutation = torch.randperm(len(states))\n",
    "\n",
    "        for i in range(0,len(states), batch_size):\n",
    "\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            old_probs_batch, states_batch,actions_batch,rewards_batch = old_probs_1[indices], states_1[indices],actions_1[indices],rewards[indices]\n",
    "\n",
    "            L_1 = -clipped_surrogate(policy_1, old_probs_batch, states_batch, actions_batch, rewards_batch, discount_rate, epsilon, var, 1)\n",
    "\n",
    "       \n",
    "\n",
    "            optimizer_1.zero_grad()\n",
    "            L_1.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(policy_1.parameters(), 1.0)\n",
    "            optimizer_1.step()\n",
    "            del L_1\n",
    "    \n",
    "    for _ in range(SGD_epoch_2):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        L = critic_loss(discount_scores, value_function, torch.cat((states,states_1),1))\n",
    "\n",
    "        optimizer_critic.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer_critic.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    if var > 0.2:\n",
    "        var*=0.9999\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if ((e+1)*num_traj)%50 ==0 :\n",
    "        print(\"50 game average: {1:f}\".format((e+1)*num_traj,np.mean(mean_rewards[-50:])))\n",
    "        print(np.mean(mean_rewards))\n",
    "        SGD_epoch_2 = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2290527b278>]"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([np.mean(mean_rewards[i:i+50]) for i in range(len(mean_rewards[:-50:]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running trained Agent\n",
    "for i in range(5):\n",
    "    old_probs_, old_probs_1_, states_, states_1_, actions_, actions_1_, rewards_, scores_,a = collect_trajectories(env, policy, policy_1, tmax, 2.0, training_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
