{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "The program uses PPO, an actor critic method\n",
    "\n",
    "## Proximal Policy Optimization\n",
    "Proximal Policy Optimization, or PPO, is one of the most widely used Reinforcement Learning algorithms. It is both simple, and highly effective.\n",
    "\n",
    "PPO uses two neural networks, one is called the Actor, given a state, it returns the probability that the agent will take each action. The second neural network is called the Critic which takes a state, and returns the expected reward for said state. By combining these two networks, the Critic can help tell the Actor whether it took a good or bad action, allowing the program to learn faster.\n",
    "\n",
    "## Multi-Agent version\n",
    "In this version, a single policy and critic were used. The same policy was used for both agents. It was fed the state of one agent, and gave out a mean. A random actions was sampled from a normal distribution created from this mean, and a given standard deviation. The critic took in the states of both actors, and outputed 2 expected rewards, one for each agent.\n",
    "\n",
    "## Trajectories\n",
    "In order to train, PPO must first collect a series of trajectories. The complete set of state, actions, and rewards for an episode played by the agent. In general, the more trajectories collected before the agent is updated, the faster the agent will learn\n",
    "\n",
    "## Clipped Loss Function\n",
    "\n",
    "One problem with almost all Reinforcement Learning algorithms is that they are highly unstable. When a large update is made to the policy, there is a chance for the change to be too big and greatly harm the performance. To fix this, PPO uses the clipped loss function shown below.\n",
    "\n",
    "$$min((\\frac{\\pi(a|s)}{\\pi_{old}(a|s)})A(s,a),clip(\\frac{\\pi(a|s)}{\\pi_{old}(a|s)},1-\\varepsilon,1+\\varepsilon)A(s,a))$$\n",
    "\n",
    "PPO introduces a clipping term to the loss function, taking away the incentive to change the loss function too much. This way, if the ratio of the current action probability over the old action probability is greater than 1 + epsilon, or less than 1 - epsilon, the loss function will not encourage the policy to increase or decrease respectively.\n",
    "\n",
    "In continuous action spaces, such as this environment, the probability density function is used rather than just calculating the probability. The reason for this is because the probability of a specific value being picked from any range is 0 due to the infinate amount of choices.\n",
    "\n",
    "## Generalized advantage estimation\n",
    "In the loss function showed above, the Advantage function was shown, in the version of PPO I implemented, I used Generalized Advantage Estimation in order to calculate the advantage.\n",
    "\n",
    "Generalized advantage estimation is method for estimating an advantage function that combines the discounted rewards, which usually vary greatly but are generally accurate, with the outputs from the Critic Network, which vary much less, but are less accurate than the discounted rewards. It has been shown that by finding a balance between accuracy, and variance, it is possible for a reinforcement learning program to train much faster.\n",
    "\n",
    "## Continuous Action Space\n",
    "This environment used a continuous action space. The two actions can be any value between -1 and 1. Due to this, the Actor networks had to output the mean of a multivariate distribution which was used to come up with values for the action, and to calculate the probability density for the loss function. The variance for this distribution was adjusted to cocntrol the exploration of the agent.\n",
    "\n",
    "## Network Architecture\n",
    "The Policy Network has 2 hidden layers, both with a relu activation and 128 neurons. Its input is size 24 vector, and its output is a size 2 vector. The output layer has a tanh activation\n",
    "\n",
    "The Critic Network is the same, but it has an input size of 48 as it takes in the state of both agents. The Critic Network's output layer also has a tanh activation. This was done because each episode was cut before the discounted rewards could approach 1.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Actor Learning Rate: 0.0003, getting multiplied by 0.999 every episode\n",
    "\n",
    "Critic Learning Rate: 0.01, getting multiplied by 0.999 every episode\n",
    "\n",
    "Update Frequency: Every episode\n",
    "\n",
    "Actor batch size: 32\n",
    "\n",
    "Actor epochs: 10\n",
    "\n",
    "Critic batch size: 32\n",
    "\n",
    "Critic epochs: 30\n",
    "\n",
    "Epsilon: 0.15, multiplied by 0.999 until it hit 0.05\n",
    "\n",
    "Discount: 0.99\n",
    "\n",
    "Residual Discount: 0.96(Used in Generalized advantage estimation)\n",
    "\n",
    "\n",
    "## Learning\n",
    "The agent took 27021 episodes to solve the environment(the average reward between 27021 and 27121 is 0.53). As shown in the graph below, the reward increased gradually, but then made a sharp spike towards the end. The agent likely would have continued to increase its reward had it kept being trained. As can be seen when running the trained agent with little exploration, it is able to keep playing for a long time without dropping the ball.\n",
    "\n",
    "![alt text](learning_curve.png \"learning_curve\")\n",
    "\n",
    "## Ideas for Future Work\n",
    "It would be interesting to see what the effect of using PPO-penalty, which uses KL divergence rather than the clipping that was used in this implementation. The hyperparameters could also be experimented with to see wheter they could help speed up training. This agent was very innefficient. By tuning the hyperparameters, it is very likely that it would be able to train much faster. If the learning rate and variance had been annealed faster, the agent probably would have increased its reward faster as well.\n",
    "\n",
    "Other algorithms could be tried as well, such as TRPO and DDPG to compare both the sample efficiency, and the runtime for each different algorithm. While PPO generally is faster than DDPG, DDPG is oftentimes more sample efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
